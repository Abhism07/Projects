{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d191f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Context: Python is a scripting language used for programming and data analytics purposes. It is the best programming language founded by Guido van Rossum.\n",
      "Generated Question: question: What is the name of the best programming language?\n",
      "Extracted Answer: guido van rossum.\n",
      "\n",
      "Original Context: Python is a scripting language used for programming and data analytics purposes. It is the best programming language founded by Guido van Rossum.\n",
      "Generated Question: question: What is the name of the best programming language?\n",
      "Extracted Answer: guido van rossum.\n",
      "\n",
      "Original Context: Python is a scripting language used for programming and data analytics purposes. It is the best programming language founded by Guido van Rossum.\n",
      "Generated Question: question: What is the name of the best programming language?\n",
      "Extracted Answer: guido van rossum.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load T5 model for question generation\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "# Load BERT model for answer extraction\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def generate_and_extract(context, max_length=64):\n",
    "    # Generate question using T5\n",
    "    input_text_t5 = \"context: %s </s>\" % (context)\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(input_ids=features_t5['input_ids'], \n",
    "                                  attention_mask=features_t5['attention_mask'],\n",
    "                                  max_length=max_length)\n",
    "    generated_question = tokenizer_t5.decode(output_t5[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract answer using BERT\n",
    "    extracted_answer = extract_answer(generated_question, context)\n",
    "    \n",
    "    return generated_question, extracted_answer\n",
    "\n",
    "def extract_answer(question, context):\n",
    "    inputs = tokenizer_bert(question, context, return_tensors='pt')\n",
    "    start_scores, end_scores = model_bert(**inputs).start_logits, model_bert(**inputs).end_logits\n",
    "    start_index = torch.argmax(start_scores, dim=1).item()\n",
    "    end_index = torch.argmax(end_scores, dim=1).item() + 1\n",
    "    answer_span = inputs['input_ids'][0][start_index:end_index]\n",
    "    answer = tokenizer_bert.decode(answer_span, skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = \"Python is a scripting language used for programming and data analytics purposes. It is the best programming language founded by Guido van Rossum.\"\n",
    "answers = [\"data analytics\", \"programming\", \"Guido van Rossum\"]\n",
    "\n",
    "for answer in answers:\n",
    "    generated_question, extracted_answer = generate_and_extract(context)\n",
    "    print(f\"Original Context: {context}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Extracted Answer: {extracted_answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ec33be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions: ['question: What is a backtracking technique used for?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load T5 model for question generation\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "# Load BERT model for answer extraction (if needed)\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def generate_questions(context, max_questions=3, max_length=128, num_beams=5, no_repeat_ngram_size=2):\n",
    "    \"\"\"Generates multiple questions from a given context.\"\"\"\n",
    "\n",
    "    input_text_t5 = f\"Generate {max_questions} questions about the following context: {context} </s>\"\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size\n",
    "    )\n",
    "    generated_questions = tokenizer_t5.decode(output_t5[0], skip_special_tokens=True).split(\"|||\")[:max_questions]\n",
    "\n",
    "    return generated_questions\n",
    "\n",
    "# Example usage\n",
    "context = \"Backtracking is a problem-solving algorithmic technique that involves finding a solution incrementally by trying different options and undoing them if they lead to a dead end. It is commonly used in situations where you need to explore multiple possibilities to solve a problem, like searching for a path in a maze or solving puzzles like Sudoku. When a dead end is reached, the algorithm backtracks to the previous decision point and explores a different path until a solution is found or all possibilities have been exhausted.\"\n",
    "generated_questions = generate_questions(context)\n",
    "print(f\"Generated Questions: {generated_questions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c14850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions: ['question: What is the Stanford Question Answering Dataset?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import random\n",
    "# Load T5 model for summarization (finetuned for questions)\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "def generate_multi_questions(context, num_questions=3, max_length=64, temperature=0.8):\n",
    "    \"\"\"Generates multiple diverse questions from a given context.\"\"\"\n",
    "\n",
    "    # Prompt with variable format and optional keywords\n",
    "    prompts = [\n",
    "        f\"Summarize the key points of {context} in the form of {num_questions} questions.\",\n",
    "        f\"Generate {num_questions} open-ended questions about {context}.\",\n",
    "        f\"What are {num_questions} interesting things to ask about {context}?\",\n",
    "    ]\n",
    "    input_text_t5 = random.choice(prompts) + \" </s>\"\n",
    "\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    generated_text = tokenizer_t5.decode(output_t5[0], skip_special_tokens=True)\n",
    "\n",
    "    # Split and filter generated questions\n",
    "    questions = generated_text.strip().split(\". \")\n",
    "    questions = [q for q in questions if q][:num_questions]\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "#context = \"Python is a scripting language used for programming and data analytics purposes. It is the best programming language founded by Guido van Rossum.\"\n",
    "context = \"The Stanford Question Answering Dataset (SQuAD 1.1) is a popular dataset for training and evaluating question-answering models. It contains more than 100,000 question-answer pairs, each consisting of a question about a passage of text and the corresponding answer. The dataset is widely used in natural language processing research, and is considered to be a benchmark for question-answering performance.\"\n",
    "\n",
    "generated_questions = generate_multi_questions(context)\n",
    "print(f\"Generated Questions: {generated_questions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10076781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "['Question: <pad> question: []</s>', 'Question: <pad> question: []</s>', 'Question: <pad> question: []</s>']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load T5 model for summarization (finetuned for questions)\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "def generate_multi_questions(context, num_questions=3, max_length=128, temperature=0.7):\n",
    "    \"\"\"Generates multiple diverse questions from a given context.\"\"\"\n",
    "\n",
    "    # Prompts with explicit request for questions\n",
    "    prompts = [\n",
    "        f\"Please formulate {num_questions} questions that highlight the key points of {context}.\",\n",
    "        f\"Generate {num_questions} thought-provoking questions based on the following information: {context}\",\n",
    "        f\"What are the most interesting questions one could ask about {context}? Please provide {num_questions} examples.\"\n",
    "    ]\n",
    "    input_text_t5 = random.choice(prompts) + \" </s>\"\n",
    "\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    generated_text = tokenizer_t5.decode(output_t5[0], skip_special_tokens=True)\n",
    "\n",
    "    # Split by question marks and filter\n",
    "    questions = [q.strip() for q in generated_text.split(\"?\") if q.strip()]\n",
    "    questions = [q for q in questions if q.endswith(\"?\")][:num_questions]\n",
    "\n",
    "    return questions\n",
    "\n",
    "def generate_formatted_questions(context, num_questions=3, max_length=128, temperature=0.7):\n",
    "    \"\"\"Generates multiple questions in specified format from a given context.\"\"\"\n",
    "\n",
    "    # Generate questions\n",
    "    questions = generate_multi_questions(context, num_questions, max_length, temperature)\n",
    "\n",
    "    # Format output\n",
    "    formatted_questions = []\n",
    "    for i in range(1, num_questions + 1):\n",
    "        formatted_questions.append(f\"Question: <pad> question: {questions}</s>\")\n",
    "\n",
    "    return formatted_questions\n",
    "\n",
    "# Example usage\n",
    "context = \"The Stanford Question Answering Dataset (SQuAD 1.1) is a popular dataset for training and evaluating question-answering models. It contains more than 100,000 question-answer pairs, each consisting of a question about a passage of text and the corresponding answer. The dataset is widely used in natural language processing research, and is considered to be a benchmark for question-answering performance.\"\n",
    "\n",
    "formatted_questions = generate_formatted_questions(context)\n",
    "print(f\"Generated Questions:\\n{formatted_questions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545e962",
   "metadata": {},
   "source": [
    "# Important Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0dd70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "# Load BERT model and tokenizer for key point extraction\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Load T5 model for question generation\n",
    "t5_model_name = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(t5_model_name)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "    \"\"\"Generate a question using T5 with BERT-extracted key points.\"\"\"\n",
    "    # Extract key points using BERT\n",
    "    #key_points = extract_key_points(context)\n",
    "\n",
    "    # Use key points along with answer and context for T5 input\n",
    "    input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    return tokenizer_t5.decode(output_t5[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "813ec1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98bd5083d3c43e6a7c92c6a79276cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Sentences: ['public databases are often repositories of data that are accessible to the general public', 'an open-data website provides datasets that are available for public use', 'external data typically refers to information obtained from sources outside of the organization .']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from summarizer import Summarizer\n",
    "# Load BERT model for summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "model = Summarizer()\n",
    "\n",
    "def extract_sentences(context, max_length=60):\n",
    "    \"\"\"Extract important sentences using BERT-based summarization.\"\"\"\n",
    "    summary = summarizer(context, max_length=max_length, min_length=50, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
    "    #summary = model(context, num_sentences=5, min_length=60)\n",
    "     \n",
    "    # Split the summary into sentences\n",
    "    sentences = summary.split(\". \")\n",
    "    \n",
    "    # Remove empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Example usage\n",
    "#context = \"The Stanford Question Answering Dataset (SQuAD 1.1) is a popular dataset for training and evaluating question-answering models. It contains more than 100,000 question-answer pairs, each consisting of a question about a passage of text and the corresponding answer. The dataset is widely used in natural language processing research, and is considered to be a benchmark for question-answering performance.\"\n",
    "context = \"A public database: Public databases are often repositories of data that are accessible to the general public.An open-data website: Open-data websites provide datasets that are available for public use and can be a valuable resource for external data.Accessing data from the Sewati Financial Services website or its database in the cloud may not be considered external data, as it pertains specifically to Sewati Financial Services. External data typically refers to information obtained from sources outside of the organization.\"\n",
    "extracted_sentences = extract_sentences(context)\n",
    "print(f\"Extracted Sentences: {extracted_sentences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee7c76b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  What is a public database? \n",
      " Answers:public databases are often repositories of data that are accessible to the general public\n",
      "  What is an open-data website? \n",
      " Answers:an open-data website provides datasets that are available for public use\n",
      "  What is external data? \n",
      " Answers:external data typically refers to information obtained from sources outside of the organization .\n"
     ]
    }
   ],
   "source": [
    "def remove_special_tokens(question):\n",
    "    cleaned_question = question.replace('question:','').replace('<pad>', '').replace('</s>', '')\n",
    "    return cleaned_question\n",
    "\n",
    "# Example usage:\n",
    "question=[]\n",
    "#cleaned_question = remove_special_tokens(generated_question)\n",
    "\n",
    "for i in extracted_sentences:\n",
    "    generated_question = get_question(i, context)\n",
    "    rem=remove_special_tokens(generated_question)\n",
    "    question.append(rem)\n",
    "    print(f\"{rem} \\n Answers:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ab150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  What is a public database?', '  What is a public database?', '  What is the purpose of an open-data website?', '  What is external data?']\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ba643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import tensorflow as tf  # Or any other framework you're using with T5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59708973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.404215963202711e-231\n"
     ]
    }
   ],
   "source": [
    "generated_questions =['What is a valuable resource for external data?',\n",
    "                      'What is a public database?', \n",
    "                      'What is the term for information obtained from outside of the organization?']\n",
    "\n",
    "reference_questions = [\n",
    " ' What is a valuable resource for external data?',\n",
    "                      '  What is a public database?', \n",
    "                      '  What is the term for information obtained from outside of the organization?'\n",
    "]\n",
    "\n",
    "bleu_score = corpus_bleu(reference_questions, generated_questions)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "914ea212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "def read_text(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "for i in extracted_sentences:\n",
    "    generated_question = get_question(i, context)\n",
    "    read_text(f\"{remove_special_tokens(generated_question)} \\n Answers:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09d95b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What type of website provides datasets that are available for public use? \n",
      " Answers: An open-data website: Open-data websites provide datasets that are available for public use\n",
      "\n",
      "\n",
      "question: What is a repository of data that is accessible to the general public? \n",
      " Answers: A public database: Public databases are often repositories of data that are accessible to the general public\n",
      "\n",
      "\n",
      "question: What does external data refer to? \n",
      " Answers: External data: External data typically refers to information obtained from sources outside of the organization.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "# Load T5 model for question generation\n",
    "t5_model_name = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(t5_model_name)\n",
    "\n",
    "def get_beam_questions(answer, context, num_beams=5, max_length=64):\n",
    "    \"\"\"Generate questions using beam search.\"\"\"\n",
    "    input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "\n",
    "    # Use num_beams for beam search\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=tokenizer_t5.encode(input_text_t5, return_tensors='pt'),\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "\n",
    "    # Decode the top-k sequences\n",
    "    generated_questions = [tokenizer_t5.decode(output, skip_special_tokens=True) for output in output_t5]\n",
    "\n",
    "    return generated_questions\n",
    "\n",
    "# Example usage\n",
    "for i in extracted_sentences:\n",
    "    generated_questions = get_beam_questions(i, context)\n",
    "    \n",
    "    # Print the top questions generated by beam search\n",
    "    for idx, question in enumerate(generated_questions):\n",
    "        print(f\"{remove_special_tokens(question)} \\n Answers: {i}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69864501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import pyttsx3\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load BERT model for summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "#summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# Load T5 model for question generation\n",
    "t5_model_name = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(t5_model_name)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "\n",
    "def remove_special_tokens(question):\n",
    "    cleaned_question = question.replace('<pad>', '').replace('</s>', '')\n",
    "    return cleaned_question\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "    input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    return tokenizer_t5.decode(output_t5[0])\n",
    "\n",
    "# def read_text(text):\n",
    "#     engine.say(text)\n",
    "#     engine.runAndWait()\n",
    "question=[]\n",
    "def generate_questions():\n",
    "    context = entry.get()\n",
    "    \n",
    "    if context:\n",
    "        extracted_sentences = extract_sentences(context)\n",
    "        \n",
    "        result_text.config(state=tk.NORMAL)\n",
    "        result_text.delete(1.0, tk.END)  # Clear previous results\n",
    "\n",
    "        for i in extracted_sentences:\n",
    "                generated_question = get_question(i, context)\n",
    "                question.append(generated_question)\n",
    "                result_text.insert(tk.END, f\"{remove_special_tokens(generated_question)} \\n Answer: {i}\\n\")\n",
    "#             read_text(f\"{remove_special_tokens(generated_question)} \\n Answer: {i}\")\n",
    "\n",
    "        result_text.insert(tk.END, \"\\n\\n\")\n",
    "        result_text.update_idletasks()  # Update the widget to immediately display changes\n",
    "\n",
    "# Replace this with your sentence extraction logic\n",
    "def extract_sentences(context, max_length=100):\n",
    "    # Your implementation here\n",
    "    summary = summarizer(context, max_length=max_length, min_length=60, length_penalty=1.0, num_beams=8)[0]['summary_text']\n",
    "    \n",
    "    # Split the summary into sentences\n",
    "    sentences = summary.split(\". \")\n",
    "    \n",
    "    # Remove empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    return sentences\n",
    "    pass\n",
    "\n",
    "# Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Question Generation App\")\n",
    "# for i in extracted_sentences:\n",
    "#     read_text(f\"{remove_special_tokens(generated_question)} \\n Answer: {i}\")\n",
    "# Create GUI components (entry, button, text widget)\n",
    "entry = tk.Entry(root, width=50)\n",
    "generate_button = tk.Button(root, text=\"Generate Questions\", command=generate_questions)\n",
    "result_text = tk.Text(root, wrap=tk.WORD, height=20, width=60)\n",
    "result_text.config(state=tk.DISABLED)\n",
    "\n",
    "# Place GUI components in the window\n",
    "entry.pack(pady=10)\n",
    "generate_button.pack(pady=10)\n",
    "result_text.pack(pady=10)\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76e51d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<pad> question: What is India's ranking in terms of population?</s>\", \"<pad> question: What is India's border with?</s>\", '<pad> question: Where is India located in the Indian Ocean?</s>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import tensorflow as tf  # Or any other framework you're using with T5\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52e4d4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.2857142857142857, 'p': 0.4, 'f': 0.33333332847222225}, 'rouge-2': {'r': 0.14285714285714285, 'p': 0.2222222222222222, 'f': 0.1739130387145559}, 'rouge-l': {'r': 0.2857142857142857, 'p': 0.4, 'f': 0.33333332847222225}}, {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}, {'rouge-1': {'r': 0.375, 'p': 0.3, 'f': 0.33333332839506175}, 'rouge-2': {'r': 0.14285714285714285, 'p': 0.1111111111111111, 'f': 0.12499999507812519}, 'rouge-l': {'r': 0.375, 'p': 0.3, 'f': 0.33333332839506175}}]\n"
     ]
    }
   ],
   "source": [
    "generated_questions = question\n",
    "\n",
    "reference_questions =[\n",
    "'As of June 2023, what is India rank in terms of population among the countries?','Which countries share land borders with India to the west?',\n",
    "'Which ocean is in the vicinity of India?']\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "# Sample generated and reference summaries\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(generated_questions, reference_questions)\n",
    "print(scores,end='\\n')\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# bleu_score = corpus_bleu(reference_questions, generated_questions)\n",
    "# print(bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85273600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<pad> question: What is India's ranking in terms of population?</s>\", \"<pad> question: What is India's border with?</s>\", '<pad> question: Where is India located in the Indian Ocean?</s>']\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2373cf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0\n",
      "BLEU-2: 0\n",
      "BLEU-3: 0\n",
      "BLEU-4: 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Reference and hypothesis sentences\n",
    "generated_questions = question\n",
    "\n",
    "reference_questions = [\n",
    "    'As of June 2023, what is India rank in terms of population among the countries?',\n",
    "    'Which countries share land borders with India to the west?',\n",
    "    'Which ocean is in the vicinity of India?'\n",
    "]\n",
    "\n",
    "# Convert the reference questions to a list of lists of tokens\n",
    "reference_questions_tokenized = [ref.split() for ref in reference_questions]\n",
    "\n",
    "# Convert the generated questions to a list of tokens\n",
    "generated_questions_tokenized = [gen.split() for gen in generated_questions]\n",
    "\n",
    "# Convert lists of tokens to tuples\n",
    "reference_questions_tokenized = [tuple(tokens) for tokens in reference_questions_tokenized]\n",
    "generated_questions_tokenized = [tuple(tokens) for tokens in generated_questions_tokenized]\n",
    "\n",
    "# Calculate BLEU scores for n-grams 1 to 4\n",
    "weights = [(1, 0, 0, 0), (0.5, 0.5, 0, 0), (1/3, 1/3, 1/3, 0), (0.25, 0.25, 0.25, 0.25)]\n",
    "bleu_scores = [corpus_bleu([reference_questions_tokenized], [generated_questions_tokenized], weights=weight) for weight in weights]\n",
    "\n",
    "# Print BLEU scores\n",
    "for i, score in enumerate(bleu_scores, start=1):\n",
    "    print(f\"BLEU-{i}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f84de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load BERT model for summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-large\")\n",
    "\n",
    "# Load T5 large model for question generation\n",
    "t5_model_name = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(t5_model_name)\n",
    "\n",
    "def remove_special_tokens(question):\n",
    "    cleaned_question = question.replace('<pad>', '').replace('</s>', '')\n",
    "    return cleaned_question\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "    input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    return tokenizer_t5.decode(output_t5[0])\n",
    "\n",
    "question = []\n",
    "\n",
    "def generate_questions():\n",
    "    context = entry.get()\n",
    "    \n",
    "    if context:\n",
    "        extracted_sentences = extract_sentences(context)\n",
    "        \n",
    "        result_text.config(state=tk.NORMAL)\n",
    "        result_text.delete(1.0, tk.END)  # Clear previous results\n",
    "\n",
    "        for i in extracted_sentences:\n",
    "            generated_question = get_question(i, context)\n",
    "            question.append(generated_question)\n",
    "            result_text.insert(tk.END, f\"{remove_special_tokens(generated_question)} \\n Answer: {i}\\n\")\n",
    "\n",
    "        result_text.insert(tk.END, \"\\n\\n\")\n",
    "        result_text.update_idletasks()  # Update the widget to immediately display changes\n",
    "\n",
    "def extract_sentences(context, max_length=100):\n",
    "    summary = summarizer(context, max_length=max_length, min_length=60, length_penalty=1.0, num_beams=8)[0]['summary_text']\n",
    "    sentences = summary.split(\". \")\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Tkinter GUI\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Question Generation App\")\n",
    "\n",
    "# Create GUI components (entry, button, text widget)\n",
    "# entry = tk.Entry(root, width=50)\n",
    "# generate_button = tk.Button(root, text=\"Generate Questions\", command=generate_questions)\n",
    "# result_text = tk.Text(root, wrap=tk.WORD, height=20, width=60)\n",
    "# result_text.config(state=tk.DISABLED)\n",
    "\n",
    "# # Place GUI components in the window\n",
    "# entry.pack(pady=10)\n",
    "# generate_button.pack(pady=10)\n",
    "# result_text.pack(pady=10)\n",
    "\n",
    "# # Start the Tkinter event loop\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a17fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db586f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc7c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Your max_length is set to 150, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 150, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 150, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "# Ignore specific warning category\n",
    "warnings.simplefilter(\"ignore\")\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "\n",
    "# Load T5 large model for question generation\n",
    "t5_model_name =\"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(t5_model_name)\n",
    "\n",
    "def remove_special_tokens(question):\n",
    "    cleaned_question = question.replace('<pad>', '').replace('</s>', '')\n",
    "    return cleaned_question\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "    input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    return tokenizer_t5.decode(output_t5[0])\n",
    "\n",
    "question = []\n",
    "\n",
    "def generate_questions():\n",
    "    context = entry.get()\n",
    "    \n",
    "    if context:\n",
    "        #extracted_sentences = extract_sentences(context)\n",
    "        extracted_sentences = extract_important_sentences(context)\n",
    "        result_text.config(state=tk.NORMAL)\n",
    "        result_text.delete(1.0, tk.END)  # Clear previous results\n",
    "\n",
    "        for i in extracted_sentences:\n",
    "            generated_question = get_question(i, context)\n",
    "            question.append(generated_question)\n",
    "            #result_text.insert(tk.END, f\"{remove_special_tokens(generated_question)} \\n Answer: {i}\\n\")\n",
    "            result_text.insert(tk.END, f\"{remove_special_tokens(generated_question)}\")\n",
    "\n",
    "        result_text.insert(tk.END, \"\\n\\n\")\n",
    "        result_text.update_idletasks()  # Update the widget to immediately display changes\n",
    "\n",
    "def extract_sentences(context, max_length=100):\n",
    "    summary = summarizer(context, max_length=max_length, min_length=60, length_penalty=1.0, num_beams=8)[0]['summary_text']\n",
    "    sentences = summary.split(\". \")\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def extract_important_sentences(context, chunk_size=200):\n",
    "    chunks = [context[i:i+chunk_size] for i in range(0, len(context), chunk_size)]\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=150, min_length=30, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "# Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Question Generation App\")\n",
    "\n",
    "# Create GUI components (entry, button, text widget)\n",
    "entry = tk.Entry(root, width=50)\n",
    "generate_button = tk.Button(root, text=\"Generate Questions\", command=generate_questions)\n",
    "result_text = tk.Text(root, wrap=tk.WORD, height=20, width=60)\n",
    "result_text.config(state=tk.DISABLED)\n",
    "\n",
    "# Place GUI components in the window\n",
    "entry.pack(pady=10)\n",
    "generate_button.pack(pady=10)\n",
    "result_text.pack(pady=10)\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce602eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<pad> question: What was the name of Sambhaji's family?</s>\", '<pad> question: Why was Sambhaji sent to live with Raja Jai Singh I?</s>', '<pad> question: What happened to Sambhaji as a result of the treaty?</s>', '<pad> question: What happened to the two people who escaped?</s>', '<pad> question: What did Aurangzeb do after initially refusing to recognize the title of Raja?</s>', '<pad> question: What did Shivaji do to Sambhaji?</s>', '<pad> question: What did Sambhaji do with Prataprao Gujar?</s>', '<pad> question: What happened to Sambhaji after he visited prince Muazzam?</s>', '<pad> question: What did the Marathas do during this period?</s>']\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f742c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.7142857142857143, 'p': 0.7142857142857143, 'f': 0.7142857092857143}, 'rouge-2': {'r': 0.5, 'p': 0.5, 'f': 0.4999999950000001}, 'rouge-l': {'r': 0.7142857142857143, 'p': 0.7142857142857143, 'f': 0.7142857092857143}}]\n",
      "[{'rouge-1': {'r': 0.6923076923076923, 'p': 0.8181818181818182, 'f': 0.7499999950347224}, 'rouge-2': {'r': 0.5833333333333334, 'p': 0.7, 'f': 0.6363636314049588}, 'rouge-l': {'r': 0.6923076923076923, 'p': 0.8181818181818182, 'f': 0.7499999950347224}}]\n",
      "[{'rouge-1': {'r': 0.2, 'p': 0.2, 'f': 0.19999999500000015}, 'rouge-2': {'r': 0.1111111111111111, 'p': 0.1111111111111111, 'f': 0.11111110611111134}, 'rouge-l': {'r': 0.2, 'p': 0.2, 'f': 0.19999999500000015}}]\n",
      "[{'rouge-1': {'r': 0.45454545454545453, 'p': 0.625, 'f': 0.5263157845983379}, 'rouge-2': {'r': 0.09090909090909091, 'p': 0.14285714285714285, 'f': 0.11111110635802489}, 'rouge-l': {'r': 0.36363636363636365, 'p': 0.5, 'f': 0.42105262670360116}}]\n",
      "[{'rouge-1': {'r': 0.4666666666666667, 'p': 0.5384615384615384, 'f': 0.4999999950255103}, 'rouge-2': {'r': 0.35714285714285715, 'p': 0.4166666666666667, 'f': 0.38461537964497045}, 'rouge-l': {'r': 0.3333333333333333, 'p': 0.38461538461538464, 'f': 0.35714285216836733}}]\n",
      "[{'rouge-1': {'r': 0.6, 'p': 0.5, 'f': 0.5454545404958678}, 'rouge-2': {'r': 0.25, 'p': 0.2, 'f': 0.22222221728395072}, 'rouge-l': {'r': 0.6, 'p': 0.5, 'f': 0.5454545404958678}}]\n",
      "[{'rouge-1': {'r': 0.5, 'p': 0.5714285714285714, 'f': 0.5333333283555556}, 'rouge-2': {'r': 0.2857142857142857, 'p': 0.3333333333333333, 'f': 0.3076923027218935}, 'rouge-l': {'r': 0.25, 'p': 0.2857142857142857, 'f': 0.266666661688889}}]\n",
      "[{'rouge-1': {'r': 0.2222222222222222, 'p': 0.2222222222222222, 'f': 0.22222221722222232}, 'rouge-2': {'r': 0.1, 'p': 0.125, 'f': 0.11111110617283973}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.2222222222222222, 'f': 0.22222221722222232}}]\n",
      "[{'rouge-1': {'r': 0.6, 'p': 0.75, 'f': 0.6666666617283951}, 'rouge-2': {'r': 0.2, 'p': 0.2857142857142857, 'f': 0.23529411280276827}, 'rouge-l': {'r': 0.6, 'p': 0.75, 'f': 0.6666666617283951}}]\n"
     ]
    }
   ],
   "source": [
    "generated_questions = [\n",
    "    \"What was the name of Sambhaji's family?\",\n",
    "    \"Why was Sambhaji sent to live with Raja Jai Singh I?\",\n",
    "    \"What happened to Sambhaji as a result of the treaty?\",\n",
    "    \"What happened to the two people who escaped?\",\n",
    "    \"What did Aurangzeb do after initially refusing to recognize the title of Raja?\",\n",
    "    \"What did Shivaji do to Sambhaji?\",\n",
    "    \"What did Sambhaji do with Prataprao Gujar?\",\n",
    "    \"What happened to Sambhaji after he visited prince Muazzam?\",\n",
    "    \"What did the Marathas do during this period?\"\n",
    "]\n",
    "\n",
    "reference_questions = [\n",
    "    \"What was the family name of Sambhaji?\",\n",
    "    \"For what reason was Sambhaji sent to reside with Raja Jai Singh I?\",\n",
    "    \"As an outcome of the treaty, what fate befell Sambhaji?\",\n",
    "    \"What was the fate of the two individuals who managed to escape?\",\n",
    "    \"Following his initial refusal to recognize the title of Raja, what actions did Aurangzeb take?\",\n",
    "    \"How did Shivaji treat Sambhaji?\",\n",
    "    \"with Prataprao Gujar, what actions did Sambhaji undertake?\",\n",
    "    \"Subsequent to his visit to Prince Muazzam, what happened to Sambhaji?\",\n",
    "    \"What were the actions of the Marathas during this specific period?\"\n",
    "]\n",
    "\n",
    "\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "for i in range(len(generated_questions)):\n",
    "    scores = rouge.get_scores(generated_questions[i], reference_questions[i])\n",
    "    print(scores,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c7f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shivaji I (Shivaji Shahaji Bhonsale; Marathi pronunciation: [ʃiʋaːd͡ʒiˑ bʱoˑs(ə)leˑ]; c. 19 February 1630 – 3 April 1680[5]) was an Indian ruler and a member of the Bhonsle Maratha clan.[6] Shivaji carved out his own independent kingdom from the declining[citation needed] Adilshahi Sultanate of Bijapur that formed the genesis of the Maratha Empire. In 1674, he was formally crowned the Chhatrapati of his realm at Raigad Fort.[7]  Over the course of his life, Shivaji engaged in both alliances and hostilities with the Mughal Empire, the Sultanate of Golkonda, the Sultanate of Bijapur and the European colonial powers. Shivaji's military forces expanded the Maratha sphere of influence, capturing and building forts, and forming a Maratha navy. Shivaji established a competent and progressive civil administration with well-structured administrative institutions. He revived ancient Hindu political traditions, court conventions and promoted the use of the Marathi and Sanskrit languages, replacing Persian at court and in administration.[7][8] Praised for his chivalrous treatment of women,[9] Shivaji employed people of all castes and religions, including Muslims[10] and Europeans, in his administration and armed forces.[11]  Shivaji's legacy was to vary by observer and time, but nearly two centuries after his death he began to take on increased importance with the emergence of the Indian independence movement, as many Indian nationalists elevated him as a proto-nationalist and hero of the Hindus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Your max_length is set to 150, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 150, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Your max_length is set to 150, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 150, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 150, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 150, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 150, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.40857593697119393\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "\n",
    "def extract_important_sentences(context, chunk_size=200):\n",
    "    chunks = [context[i:i+chunk_size] for i in range(0, len(context), chunk_size)]\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=150, min_length=30, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \". \".join(summaries)\n",
    "\n",
    "# Input your long context\n",
    "context = input()\n",
    "\n",
    "# Extract important sentences using the modified function\n",
    "important_summary = extract_important_sentences(context)\n",
    "\n",
    "# Reference summary (replace this with your actual reference summary)\n",
    "reference_summary = context\n",
    "\n",
    "# Convert the reference and generated summaries to lists of tokens\n",
    "reference_tokens = nltk.word_tokenize(reference_summary.lower())\n",
    "generated_tokens = nltk.word_tokenize(important_summary.lower())\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = nltk.translate.bleu_score.sentence_bleu([reference_tokens], generated_tokens)\n",
    "\n",
    "# Print the BLEU score\n",
    "print(f\"BLEU Score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e02bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243 1509\n"
     ]
    }
   ],
   "source": [
    "print(len(important_summary),len(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7035a504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [\n",
    "    {'rouge-1': {'r': 0.7142857142857143, 'p': 0.7142857142857143, 'f': 0.7142857092857143}, 'rouge-2': {'r': 0.5, 'p': 0.5, 'f': 0.4999999950000001}, 'rouge-l': {'r': 0.7142857142857143, 'p': 0.7142857142857143, 'f': 0.7142857092857143}},\n",
    "    {'rouge-1': {'r': 0.6923076923076923, 'p': 0.8181818181818182, 'f': 0.7499999950347224}, 'rouge-2': {'r': 0.5833333333333334, 'p': 0.7, 'f': 0.6363636314049588}, 'rouge-l': {'r': 0.6923076923076923, 'p': 0.8181818181818182, 'f': 0.7499999950347224}},\n",
    "    {'rouge-1': {'r': 0.2, 'p': 0.2, 'f': 0.19999999500000015}, 'rouge-2': {'r': 0.1111111111111111, 'p': 0.1111111111111111, 'f': 0.11111110611111134}, 'rouge-l': {'r': 0.2, 'p': 0.2, 'f': 0.19999999500000015}},\n",
    "    {'rouge-1': {'r': 0.45454545454545453, 'p': 0.625, 'f': 0.5263157845983379}, 'rouge-2': {'r': 0.09090909090909091, 'p': 0.14285714285714285, 'f': 0.11111110635802489}, 'rouge-l': {'r': 0.36363636363636365, 'p': 0.5, 'f': 0.42105262670360116}},\n",
    "    {'rouge-1': {'r': 0.4666666666666667, 'p': 0.5384615384615384, 'f': 0.4999999950255103}, 'rouge-2': {'r': 0.35714285714285715, 'p': 0.4166666666666667, 'f': 0.38461537964497045}, 'rouge-l': {'r': 0.3333333333333333, 'p': 0.38461538461538464, 'f': 0.35714285216836733}},\n",
    "    {'rouge-1': {'r': 0.6, 'p': 0.5, 'f': 0.5454545404958678}, 'rouge-2': {'r': 0.25, 'p': 0.2, 'f': 0.22222221728395072}, 'rouge-l': {'r': 0.6, 'p': 0.5, 'f': 0.5454545404958678}},\n",
    "    {'rouge-1': {'r': 0.5, 'p': 0.5714285714285714, 'f': 0.5333333283555556}, 'rouge-2': {'r': 0.2857142857142857, 'p': 0.3333333333333333, 'f': 0.3076923027218935}, 'rouge-l': {'r': 0.25, 'p': 0.2857142857142857, 'f': 0.266666661688889}},\n",
    "    {'rouge-1': {'r': 0.2222222222222222, 'p': 0.2222222222222222, 'f': 0.22222221722222232}, 'rouge-2': {'r': 0.1, 'p': 0.125, 'f': 0.11111110617283973}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.2222222222222222, 'f': 0.22222221722222232}},\n",
    "    {'rouge-1': {'r': 0.6, 'p': 0.75, 'f': 0.6666666617283951}, 'rouge-2': {'r': 0.2, 'p': 0.2857142857142857, 'f': 0.23529411280276827}, 'rouge-l': {'r': 0.6, 'p': 0.75, 'f': 0.6666666617283951}}\n",
    "]\n",
    "\n",
    "metrics = ['rouge-1', 'rouge-2', 'rouge-l']\n",
    "values = [[item[metric]['f'] for item in data] for metric in metrics]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.2\n",
    "x = range(len(data))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar([pos + width * i for pos in x], values[i], width, label=metric)\n",
    "\n",
    "ax.set_xticks([pos + width for pos in x])\n",
    "ax.set_xticklabels(range(1, len(data) + 1))\n",
    "ax.legend()\n",
    "ax.set_xlabel('Data Index')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Rouge Metrics Comparison')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd53233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Your max_length is set to 150, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 150, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 150, but your input_length is only 40. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QTextEdit, QVBoxLayout, QPushButton\n",
    "from PyQt5.QtGui import QFont\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "class QuestionGeneratorApp(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Move these class variables here\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "        self.t5_model_name = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "        self.tokenizer_t5 = AutoTokenizer.from_pretrained(self.t5_model_name)\n",
    "        self.model_t5 = AutoModelWithLMHead.from_pretrained(self.t5_model_name)\n",
    "\n",
    "        self.init_ui()\n",
    "\n",
    "    def init_ui(self):\n",
    "        self.setWindowTitle('Question Generator App')\n",
    "\n",
    "        self.text_edit = QTextEdit(self)\n",
    "        self.text_edit.setPlaceholderText('Enter context...')\n",
    "\n",
    "        self.generate_button = QPushButton('Generate Questions', self)\n",
    "        self.generate_button.clicked.connect(self.generate_questions)\n",
    "\n",
    "        self.result_label = QLabel('Generated Questions:', self)\n",
    "        self.result_text = QTextEdit(self)\n",
    "        self.result_text.setReadOnly(True)\n",
    "\n",
    "        # Set the font size for the result_text\n",
    "        font = QFont()\n",
    "        font.setPointSize(12)  # You can adjust the font size here\n",
    "        self.result_text.setFont(font)\n",
    "\n",
    "        layout = QVBoxLayout(self)\n",
    "        layout.addWidget(self.text_edit)\n",
    "        layout.addWidget(self.generate_button)\n",
    "        layout.addWidget(self.result_label)\n",
    "        layout.addWidget(self.result_text)\n",
    "\n",
    "        self.setLayout(layout)\n",
    "\n",
    "    def remove_special_tokens(self, question):\n",
    "        cleaned_question = question.replace('<pad>', '').replace('</s>', '')\n",
    "        return cleaned_question\n",
    "\n",
    "    def get_question(self, answer, context, max_length=64):\n",
    "        input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "        features_t5 = self.tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "        output_t5 = self.model_t5.generate(\n",
    "            input_ids=features_t5['input_ids'],\n",
    "            attention_mask=features_t5['attention_mask'],\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        return self.tokenizer_t5.decode(output_t5[0])\n",
    "\n",
    "    def extract_important_sentences(self, context, chunk_size=200):\n",
    "        chunks = [context[i:i+chunk_size] for i in range(0, len(context), chunk_size)]\n",
    "        summaries = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            summary = self.summarizer(chunk, max_length=150, min_length=30, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def generate_questions(self):\n",
    "        context = self.text_edit.toPlainText()\n",
    "\n",
    "        if context:\n",
    "            extracted_sentences = self.extract_important_sentences(context)\n",
    "            self.result_text.clear()\n",
    "\n",
    "            for i in extracted_sentences:\n",
    "                generated_question = self.get_question(i, context)\n",
    "                self.result_text.insertPlainText(f\"{self.remove_special_tokens(generated_question)}\\n Answer:{i}\\n\")\n",
    "\n",
    "            self.result_text.insertPlainText(\"\\n\")\n",
    "\n",
    "def main():\n",
    "    app = QApplication(sys.argv)\n",
    "    window = QuestionGeneratorApp()\n",
    "    window.resize(600, 400)\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Your max_length is set to 150, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 150, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 150, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 150, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 150, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QTextEdit, QVBoxLayout, QPushButton\n",
    "from PyQt5.QtGui import QFont\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import pyttsx3\n",
    "\n",
    "class QuestionGeneratorApp(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "        self.t5_model_name = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "        self.tokenizer_t5 = AutoTokenizer.from_pretrained(self.t5_model_name)\n",
    "        self.model_t5 = AutoModelWithLMHead.from_pretrained(self.t5_model_name)\n",
    "        self.engine = pyttsx3.init()\n",
    "\n",
    "        self.init_ui()\n",
    "\n",
    "    def init_ui(self):\n",
    "        self.setWindowTitle('Question Generator App')\n",
    "\n",
    "        self.text_edit = QTextEdit(self)\n",
    "        self.text_edit.setPlaceholderText('Enter context...')\n",
    "\n",
    "        self.generate_button = QPushButton('Generate Questions', self)\n",
    "        self.generate_button.clicked.connect(self.generate_questions)\n",
    "\n",
    "        self.speak_button = QPushButton('Speak', self)\n",
    "        self.speak_button.clicked.connect(self.speak_generated_text)\n",
    "\n",
    "        self.result_label = QLabel('Generated Questions:', self)\n",
    "        self.result_text = QTextEdit(self)\n",
    "        self.result_text.setReadOnly(True)\n",
    "\n",
    "        font = QFont()\n",
    "        font.setPointSize(12)\n",
    "        self.result_text.setFont(font)\n",
    "\n",
    "        layout = QVBoxLayout(self)\n",
    "        layout.addWidget(self.text_edit)\n",
    "        layout.addWidget(self.generate_button)\n",
    "        layout.addWidget(self.speak_button)\n",
    "        layout.addWidget(self.result_label)\n",
    "        layout.addWidget(self.result_text)\n",
    "\n",
    "        self.setLayout(layout)\n",
    "\n",
    "    def remove_special_tokens(self, question):\n",
    "        cleaned_question = question.replace('<pad>', '').replace('</s>', '')\n",
    "        return cleaned_question\n",
    "\n",
    "    def get_question(self, answer, context, max_length=64):\n",
    "        input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "        features_t5 = self.tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "        output_t5 = self.model_t5.generate(\n",
    "            input_ids=features_t5['input_ids'],\n",
    "            attention_mask=features_t5['attention_mask'],\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        return self.tokenizer_t5.decode(output_t5[0])\n",
    "\n",
    "    def extract_important_sentences(self, context, chunk_size=200):\n",
    "        chunks = [context[i:i+chunk_size] for i in range(0, len(context), chunk_size)]\n",
    "        summaries = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            summary = self.summarizer(chunk, max_length=150, min_length=30, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def generate_questions(self):\n",
    "        context = self.text_edit.toPlainText()\n",
    "\n",
    "        if context:\n",
    "            extracted_sentences = self.extract_important_sentences(context)\n",
    "            self.result_text.clear()\n",
    "\n",
    "            for i in extracted_sentences:\n",
    "                generated_question = self.get_question(i, context)\n",
    "                self.result_text.insertPlainText(f\"{self.remove_special_tokens(generated_question)}\\n Answer:{i}\\n\")\n",
    "\n",
    "            self.result_text.insertPlainText(\"\\n\")\n",
    "            self.speak_generated_text()\n",
    "\n",
    "    def speak_generated_text(self):\n",
    "        # Speak the generated text\n",
    "        self.engine.say(self.result_text.toPlainText())\n",
    "        self.engine.runAndWait()\n",
    "\n",
    "def main():\n",
    "    app = QApplication(sys.argv)\n",
    "    window = QuestionGeneratorApp()\n",
    "    window.resize(600, 400)\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db3dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "tations in\n",
      "vice versa,\n",
      "> distiner\n",
      "nerast, 2\n",
      "rembers\n",
      "happily\n",
      "1 dogs\n",
      "\n",
      "zecher\n",
      "\n",
      "gusifleance\n",
      "mal of Ne SHE\n",
      "An an\n",
      "\n",
      "41 genera). Lions, tigers, leopards and\n",
      "thin the genus Pantvert. Biologists\n",
      "t Lain name, genus followed by\n",
      "Hed Panther feo, the species feo of\n",
      "nus Panciers Presumably, everyone reading this book is 4\n",
      "the species sapiens (wise) of the genus Homo (oan)\n",
      "Genera 1m cheir caro are grouped into families, such as che cats\n",
      "tions, dhecahs, house cats), the dogs (wolves, foxes, jackals) and the\n",
      "tlephanes (elephants, mammoths, mastodons). All members of a family\n",
      "sce thei lineage back to # Founding matriarch of patriarch. All cats,\n",
      "forexample, (rom the smallest house Kieren to the most ferocious lion,\n",
      "hho lived about 25 million years ago.\n",
      "\n",
      "shace 2 common feline ancestor wl\n",
      "\"Menno sapienr, roo, belongs 10 2 family, This banal fact used co\n",
      "\n",
      "af hiscory’s rose closely guarded secrets. Homo sapiens long\n",
      "set apart from animals, an orphan who has\n",
      "and — most importantly - no parents. But\n",
      "thar jos nor the case. Like it oF not, we are members of a large and\n",
      "pariculaly noisy family called the great apes. Our nearest living\n",
      "aclide chimpanzees, gorillas and orang-utans. The chim-\n",
      "\n",
      "he heading “genus (plas\n",
      "re different species Wi\n",
      "eronganisins wich # O¥OrPAT\n",
      "\" Lions, for example, are ca\n",
      "\n",
      "ander th\n",
      "\n",
      "aguas\n",
      "\n",
      "species\n",
      "the g\n",
      "Hono sapicn ~\n",
      "\n",
      "be one\n",
      "peefered to view ise a\n",
      "\n",
      "no family, ne cousins,\n",
      "\n",
      "relatives ii\n",
      "pannees are the closest. Just 6 million years ago, a single Female ape\n",
      "\n",
      "had evo daughters. One became che ancestor of all chimpanzees, the\n",
      "\n",
      "other is our own grandmother.\n",
      "\n",
      "Skeletons in the Closet\n",
      "\n",
      "Homo sapiens has kept hidden an even more disturbing secret, No\n",
      "only do we possess an abundance of uncivilised cousins, once upon\n",
      "a time we had quite a few brothers and sisters as well. We are used\n",
      "to thinking abou ourselves as the only humans, because for the last\n",
      "10,000 years, our species has indeed been the only human species\n",
      "around. Yer the real meaning of the word human is ‘an animal\n",
      "belonging to the genus Homa’, and there used to be many other\n",
      "\n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Set the path to Tesseract executable (change it based on your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\tesseract.exe'\n",
    "\n",
    "# Read the image using OpenCV\n",
    "image_path = 'new_img.jpeg'\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply thresholding to obtain a binary image\n",
    "_, binary_image = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Perform OCR using Tesseract\n",
    "text = pytesseract.image_to_string(binary_image)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Extracted Text:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaeb85e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Your max_length is set to 150, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 150, but your input_length is only 72. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n",
      "Your max_length is set to 150, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 150, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "Your max_length is set to 150, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n",
      "Your max_length is set to 150, but your input_length is only 73. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n",
      "Your max_length is set to 150, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 150, but your input_length is only 63. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Your max_length is set to 150, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 150, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 150, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " question: What is the genus Pantvert?\n",
      " Answer:distiner nerast, 2 rembers happily 1 dogs zecher gusifleance mal of Ne SHE An an 41 genera). Lions, tigers, leopards and thin the genus Pantvert.\n",
      "\n",
      " question: What is the most common species of the clade?\n",
      " Answer:the species feo of nus Panciers is grouped into faeo . the genus Homo (oan) is a group of species that have been grouped together . cheir caro is the most common species of the clade .\n",
      "\n",
      " question: What are some of the genera 1m cheir caro grouped into families?\n",
      " Answer:che cats tions, dhecahs, house cats), the dogs (wolves, foxes, jackals) and the tlephanes (elephants, mammoths, mastodons) all members of a family sce thei lineage back to # Founding .\n",
      "\n",
      " question: Who was the founder of the family?\n",
      " Answer:matriarch of patriarch. all cats, forexample, (rom the smallest house Kieren to the most ferocious lion, hho lived about 25 million years ago . all feline ancestor wl \"Menno sapienr, roo, b\"\n",
      "\n",
      " question: What is the name of the common feline ancestor?\n",
      " Answer:elongs 10 2 family, This banal fact used co af hiscory’s rose closely guarded secrets .\n",
      "\n",
      " question: What is the name of the apes that we are members of?\n",
      " Answer:we are members of a large and pariculaly noisy family called the great apes . our nearest living aclide chimpanzees, gorillas and orang-utans .\n",
      "\n",
      " question: What is the closest family to Homo sapiens?\n",
      " Answer:lions, for example, are ca ander th aguas species the g Hono sapicn . iii ne cousins, relatives, relatives .\n",
      "\n",
      " question: How did one of the chimpanzees become ancestor of all chimpanzees?\n",
      " Answer:just 6 million years ago, a single Female ape had evo daughters . che ancestor of all chimpanzees, the other is our own grandmother .\n",
      "\n",
      " question: What has Homo sapiens kept hidden?\n",
      " Answer:o sapiens has kept an even more disturbing secret . once upon a time we had quite a few brothers and sisters . we are used to thwarting uncivilised cousins .\n",
      "\n",
      " question: Why are we so used to thinking we are the only humans around?\n",
      " Answer:for the last 10,000 years, our species has indeed been the only human species around . yer the real meaning of the word human is ‘an animal belonging t t to a group of people’ .\n",
      "\n",
      " question: What is the real meaning of the word human?\n",
      " Answer:o the genus Homa’, and there used to be many other . species . many other species were also found in the same family .\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m context \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m     61\u001b[0m questions \u001b[38;5;241m=\u001b[39m generate_questions(context)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, question \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m warnings\u001b[38;5;241m.\u001b[39mresetwarnings()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "# Ignore specific warning category\n",
    "warnings.simplefilter(\"ignore\")\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "\n",
    "# Load T5 large model for question generation\n",
    "t5_model_name =\"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "model_t5 = AutoModelWithLMHead.from_pretrained(t5_model_name)\n",
    "\n",
    "def remove_special_tokens(question):\n",
    "    cleaned_question = question.replace('<pad>', '').replace('</s>', '')\n",
    "    return cleaned_question\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "    input_text_t5 = f\"answer: {answer}  context: {context}\"\n",
    "    features_t5 = tokenizer_t5([input_text_t5], return_tensors='pt')\n",
    "    output_t5 = model_t5.generate(\n",
    "        input_ids=features_t5['input_ids'],\n",
    "        attention_mask=features_t5['attention_mask'],\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    return tokenizer_t5.decode(output_t5[0])\n",
    "\n",
    "def generate_questions(context):\n",
    "    question_list = []\n",
    "    \n",
    "    if context:\n",
    "        #extracted_sentences = extract_sentences(context)\n",
    "        extracted_sentences = extract_important_sentences(context)\n",
    "\n",
    "        for i in extracted_sentences:\n",
    "            generated_question = get_question(i, context)\n",
    "            question_list.append(remove_special_tokens(generated_question))\n",
    "            qna = print(f\"{remove_special_tokens(generated_question)}\\n Answer:{i}\\n\")\n",
    "            \n",
    "\n",
    "#         return question_list\n",
    "\n",
    "def extract_sentences(context, max_length=100):\n",
    "    summary = summarizer(context, max_length=max_length, min_length=60, length_penalty=1.0, num_beams=8)[0]['summary_text']\n",
    "    sentences = summary.split(\". \")\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def extract_important_sentences(context, chunk_size=200):\n",
    "    chunks = [context[i:i+chunk_size] for i in range(0, len(context), chunk_size)]\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=150, min_length=30, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "# Example usage:\n",
    "context = text\n",
    "questions = generate_questions(context)\n",
    "for idx, question in enumerate(questions, 1):\n",
    "    print(f\"Question {idx}: {question}\")\n",
    "\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e8980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
